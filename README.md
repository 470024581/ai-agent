# Smart AI Assistant

> Enterprise-grade intelligent data analysis platform combining natural language processing with advanced workflow orchestration

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![React](https://img.shields.io/badge/React-18.0+-blue.svg)](https://reactjs.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com)
[![LangChain](https://img.shields.io/badge/LangChain-Latest-purple.svg)](https://langchain.com)

## ğŸ¯ Project Overview

Smart AI Assistant is a sophisticated full-stack platform that demonstrates the integration of cutting-edge AI technologies with enterprise data management. The system combines React-based frontend interfaces with a powerful FastAPI backend, featuring LangGraph workflow orchestration, multi-LLM support, and real-time data processing capabilities.

### ğŸŒŸ Key Highlights

- **ğŸ§  Advanced AI Processing**: LangGraph-powered workflow orchestration with multi-step reasoning
- **ğŸ”„ Real-time Monitoring**: WebSocket-based live workflow execution tracking
- **ğŸ“Š Intelligent Data Analysis**: Natural language queries with automated chart generation
- **ğŸ—‚ï¸ Multi-source Data Integration**: SQL databases, document repositories, and hybrid data sources
- **ğŸŒ Modern Architecture**: React 18 frontend with FastAPI backend and enterprise-grade scalability
- **ğŸ”§ Multi-LLM Support**: OpenAI, OpenRouter, and Ollama integration with unified configuration

## ğŸ—ï¸ System Architecture

```mermaid
graph TB
    subgraph "Frontend Layer"
        A[React 18 Client]
        B[Redux Toolkit]
        C[Tailwind CSS + Radix UI]
        D[WebSocket Client]
    end
    
    subgraph "Backend Layer"
        E[FastAPI Server]
        F[LangGraph Engine]
        G[WebSocket Manager]
        H[Multi-LLM Factory]
    end
    
    subgraph "Data Layer"
        I[SQLite/PostgreSQL]
        J[FAISS Vector Store]
        K[File Storage]
    end
    
    subgraph "AI Services"
        L[OpenAI GPT]
        M[OpenRouter Models]
        N[Ollama Local]
        O[HuggingFace Embeddings]
    end
    
    A --> E
    D --> G
    E --> F
    F --> H
    H --> L
    H --> M
    H --> N
    E --> I
    E --> J
    E --> K
    J --> O
```

## ğŸš€ Features

### Frontend Capabilities
- **ğŸ“± Modern React Interface**: Built with React 18, Redux Toolkit, and Tailwind CSS
- **ğŸ¨ Professional UI Components**: Radix UI-based design system with dark/light theme support
- **ğŸŒ Internationalization**: Multi-language support (English/Chinese) with i18next
- **ğŸ“Š Interactive Visualizations**: Real-time charts and data dashboards
- **âš¡ Real-time Updates**: Live workflow monitoring with WebSocket integration
- **ğŸ” Node Inspection**: Detailed workflow step analysis and debugging tools

### Backend Capabilities
- **ğŸ”„ LangGraph Workflows**: Sophisticated AI processing pipelines with error recovery
- **ğŸ¤– Multi-LLM Integration**: Seamless switching between OpenAI, OpenRouter, and Ollama
- **ğŸ“ Intelligent File Processing**: Support for CSV, PDF, Word, Excel, and text documents
- **ğŸ” Hybrid Data Sources**: SQL queries, document search, and combined reasoning
- **ğŸŒ WebSocket Broadcasting**: Real-time client notifications and status updates
- **ğŸ›¡ï¸ Enterprise Security**: Comprehensive error handling and validation

## ğŸ› ï¸ Technology Stack

### Frontend Technologies
| Category | Technology | Purpose |
|----------|------------|---------|
| **Framework** | React 18 | Modern component-based UI |
| **State Management** | Redux Toolkit | Predictable state container |
| **Styling** | Tailwind CSS + Radix UI | Utility-first CSS + accessible components |
| **Build Tool** | Vite | Fast development and build |
| **Internationalization** | i18next | Multi-language support |
| **HTTP Client** | Axios | API communication |
| **Real-time** | WebSocket | Live data updates |

### Backend Technologies
| Category | Technology | Purpose |
|----------|------------|---------|
| **Framework** | FastAPI | High-performance async web framework |
| **AI Orchestration** | LangGraph | Workflow management and execution |
| **LLM Integration** | LangChain | AI model abstraction and chaining |
| **Vector Database** | FAISS | Similarity search and embeddings |
| **Database** | SQLAlchemy + SQLite/PostgreSQL | Data persistence |
| **Document Processing** | PyPDF2, python-docx, openpyxl | File parsing |
| **Embeddings** | Sentence Transformers | Local text embeddings |
| **Real-time** | WebSocket Manager | Connection management |

## ğŸ“¦ Project Structure

```
smart-ai-assistant/
â”œâ”€â”€ client/                     # React Frontend Application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx   # Main dashboard interface
â”‚   â”‚   â”‚   â”œâ”€â”€ IntelligentAnalysis.jsx # AI workflow interface
â”‚   â”‚   â”‚   â”œâ”€â”€ DataSourceManager.jsx  # Data source management
â”‚   â”‚   â”‚   â””â”€â”€ ui/            # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ hooks/             # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ store/             # Redux store configuration
â”‚   â”‚   â”œâ”€â”€ services/          # API service layer
â”‚   â”‚   â””â”€â”€ locales/           # Internationalization files
â”‚   â”œâ”€â”€ package.json           # Frontend dependencies
â”‚   â””â”€â”€ README.md              # Frontend documentation
â”œâ”€â”€ server/                     # FastAPI Backend Application
â”‚   â”œâ”€â”€ src/                   # Core application package
â”‚   â”‚   â”œâ”€â”€ agents/           # AI agent implementations
â”‚   â”‚   â”œâ”€â”€ api/              # API endpoints and routes
â”‚   â”‚   â”œâ”€â”€ chains/           # LangChain workflow definitions
â”‚   â”‚   â”œâ”€â”€ components/       # Reusable components
â”‚   â”‚   â”œâ”€â”€ config/          # Configuration management
â”‚   â”‚   â”œâ”€â”€ database/        # Database operations
â”‚   â”‚   â”œâ”€â”€ document_loaders/ # File processing and loading
â”‚   â”‚   â”œâ”€â”€ models/          # Data models and factories
â”‚   â”‚   â”œâ”€â”€ prompts/         # LLM prompts and templates
â”‚   â”‚   â”œâ”€â”€ utils/           # Utility functions
â”‚   â”‚   â”œâ”€â”€ vectorstores/    # Vector storage implementations
â”‚   â”‚   â”œâ”€â”€ websocket/       # WebSocket management
â”‚   â”‚   â””â”€â”€ main.py         # FastAPI application entry
â”‚   â”œâ”€â”€ data/                # Data storage directory
â”‚   â”‚   â”œâ”€â”€ embeddings_cache/ # Embedding model cache
â”‚   â”‚   â”œâ”€â”€ reports/        # Generated reports
â”‚   â”‚   â”œâ”€â”€ resume/         # Resume storage
â”‚   â”‚   â”œâ”€â”€ sample_sales/   # Sample data
â”‚   â”‚   â”œâ”€â”€ uploads/        # File uploads
â”‚   â”‚   â””â”€â”€ smart.db       # SQLite database
â”‚   â”œâ”€â”€ requirements.txt    # Python dependencies
â”‚   â””â”€â”€ README.md          # Backend documentation
â”œâ”€â”€ docs/                  # Project documentation
â”‚   â””â”€â”€ data/             # Sample data files
â”œâ”€â”€ package.json          # Root package configuration
â””â”€â”€ README.md            # This file
```

## ğŸš€ Quick Start

### Prerequisites

- **Node.js 16+** with npm/yarn
- **Python 3.8+** with pip
- **Git** for version control

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd smart-ai-assistant
```

2. **Install dependencies**
```bash
# Install root dependencies
npm install

# Install frontend dependencies
cd client
npm install
cd ..

# Install backend dependencies
cd server
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
cd ..
```

3. **Environment configuration**
```bash
# Backend configuration
cd server
cp env.example .env
# Edit .env with your API keys and preferences

# Frontend configuration (if needed)
cd ../client
cp .env.example .env.local
# Edit .env.local if custom configuration is needed
```

4. **Initialize the database**
```bash
cd server
python start.py
# This will create the database and initialize sample data
```

### Development Setup

1. **Start the backend server**
```bash
cd server
source venv/bin/activate  # On Windows: venv\Scripts\activate
python start.py
# Server will run on http://localhost:8000
```

2. **Start the frontend development server**
```bash
cd client
npm run dev
# Client will run on http://localhost:3000
```

3. **Access the application**
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Documentation: http://localhost:8000/docs

## ğŸ¯ Usage Examples

### Natural Language Queries

The system supports sophisticated natural language queries across different data sources:

```javascript
// Sales Analysis
"What were the total sales for this month?"
"Show me daily sales trends for the past week"
"Generate a sales performance chart"

// Inventory Management
"Which products are running low on stock?"
"List all products with inventory below 50 units"

// Document Questions (RAG)
"What does our policy document say about returns?"
"Summarize the key points from the uploaded manual"
```

### Workflow Monitoring

Track AI processing in real-time:

```javascript
// Connect to WebSocket for live updates
const ws = new WebSocket('ws://localhost:8000/ws/workflow/client-123');

ws.onmessage = (event) => {
  const update = JSON.parse(event.data);
  console.log('Workflow Progress:', update);
  // Handle workflow node updates, completion, errors
};
```

### API Integration

```bash
# Start intelligent analysis workflow
curl -X POST "http://localhost:8000/api/v1/intelligent-analysis" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Show me sales data for this quarter",
    "datasource_id": 1,
    "client_id": "client-123"
  }'

# Upload data file
curl -X POST "http://localhost:8000/api/v1/datasources/1/upload" \
  -F "file=@sales_data.csv" \
  -F "description=Q4 Sales Data"
```

## ğŸ”§ Configuration

### Backend Configuration

Key environment variables in `server/.env`:

```env
# LLM Provider (openai, openrouter, ollama)
LLM_PROVIDER=openai
LLM_MODEL=gpt-3.5-turbo
OPENAI_API_KEY=your_api_key_here

# Embedding Provider (local, openai, huggingface)
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=intfloat/multilingual-e5-small

# Database
DATABASE_URL=sqlite:///./data/smart.db

# Server
HOST=0.0.0.0
PORT=8000
DEBUG=True
```

### Frontend Configuration

Key environment variables in `client/.env.local`:

```env
VITE_API_URL=http://localhost:8000
VITE_WS_URL=ws://localhost:8000/ws
```

## ğŸ¨ Key Components

### Intelligent Analysis Interface
- **LangGraph Workflow Visualization**: Real-time workflow execution with node-by-node progress
- **Interactive Query Interface**: Natural language input with intelligent suggestions
- **Result Visualization**: Charts, tables, and formatted responses
- **Node Inspection**: Detailed view of each processing step with input/output data

### Data Source Management
- **Multi-type Support**: Knowledge bases, SQL tables, and hybrid sources
- **File Upload Interface**: Drag-and-drop with progress tracking
- **Real-time Processing**: Live status updates during file processing
- **Source Configuration**: Flexible data source setup and management

### Dashboard & Analytics
- **Executive Summary**: High-level metrics and KPIs
- **Interactive Charts**: Multiple chart types with real-time data
- **Performance Monitoring**: System health and processing statistics
- **Historical Analysis**: Trend analysis and comparative reports

## ğŸš€ Deployment

### Production Build

```bash
# Frontend production build
cd client
npm run build

# Backend production setup
cd server
pip install gunicorn
gunicorn app.main:app --bind 0.0.0.0:8000
```

### Docker Deployment

```dockerfile
# Multi-stage Dockerfile example
FROM node:18-alpine AS frontend-builder
WORKDIR /app/client
COPY client/package*.json ./
RUN npm ci
COPY client/ .
RUN npm run build

FROM python:3.11-slim AS backend
WORKDIR /app
COPY server/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY server/ .
COPY --from=frontend-builder /app/client/dist ./static
EXPOSE 8000
CMD ["python", "start.py"]
```

### Environment-Specific Configurations

- **Development**: Hot reload, debug logging, mock data
- **Staging**: Production-like environment with test data
- **Production**: Optimized builds, minimal logging, real data

## ğŸ“Š Performance & Monitoring

### Key Metrics
- **Response Time**: Average API response times
- **Workflow Duration**: End-to-end processing times
- **Concurrent Users**: Active WebSocket connections
- **Error Rates**: Failed requests and workflow errors
- **Resource Usage**: CPU, memory, and storage consumption

### Monitoring Tools
- **Built-in Logging**: Structured JSON logging with multiple levels
- **Health Checks**: Automated system health monitoring
- **WebSocket Monitoring**: Connection status and message tracking
- **Database Metrics**: Query performance and connection pooling

## ğŸ§ª Testing

### Manual Testing

```bash
# Health check
curl http://localhost:8000/api/v1/health

# WebSocket connection test
# Using websocat tool
websocat ws://localhost:8000/ws/workflow/test-client

# Frontend testing
cd client
npm run test
```

### Automated Testing

```bash
# Backend unit tests
cd server
python -m pytest tests/

# Frontend component tests
cd client
npm run test:coverage

# End-to-end testing
npm run test:e2e
```

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Follow coding standards**:
   - Frontend: ESLint + Prettier
   - Backend: Black + isort + flake8
4. **Add tests** for new functionality
5. **Update documentation** as needed
6. **Submit a Pull Request**

### Development Guidelines

- **Code Style**: Follow established patterns and conventions
- **Documentation**: Update README files and inline comments
- **Testing**: Add unit tests for new features
- **Performance**: Consider performance implications
- **Security**: Follow security best practices

## ğŸ”’ Security Considerations

- **Input Validation**: All user inputs are validated and sanitized
- **API Authentication**: Secure API key management
- **CORS Configuration**: Properly configured cross-origin requests
- **Error Handling**: Secure error messages without information leakage
- **Environment Variables**: Sensitive data stored in environment files

## ğŸ“ˆ Roadmap

### Current Features âœ…
- âœ… React 18 + Redux Toolkit frontend
- âœ… FastAPI + LangGraph backend
- âœ… Multi-LLM provider support
- âœ… Real-time WebSocket communication
- âœ… Intelligent workflow orchestration
- âœ… Document processing and RAG
- âœ… Internationalization support

### Planned Features ğŸ”„
- ğŸ”„ User authentication and authorization
- ğŸ”„ Advanced analytics dashboard
- ğŸ”„ Multi-tenant support
- ğŸ”„ Advanced caching strategies
- ğŸ”„ Mobile app development
- ğŸ”„ Integration with external APIs
- ğŸ”„ Advanced visualization options

### Future Enhancements ğŸš€
- ğŸš€ Machine learning model training
- ğŸš€ Advanced workflow templates
- ğŸš€ Plugin system architecture
- ğŸš€ Cloud deployment automation
- ğŸš€ Enterprise SSO integration

## ğŸ“š Documentation

- **[Frontend Documentation](./client/README.md)** - React application details
- **[Backend Documentation](./server/README.md)** - FastAPI service architecture  
- **[API Documentation](http://localhost:8000/docs)** - Interactive API reference
- **[Deployment Guide](./docs/deployment.md)** - Production deployment instructions

## ğŸ†˜ Support & Troubleshooting

### Common Issues

1. **Port Conflicts**: Ensure ports 3000 and 8000 are available
2. **Environment Variables**: Verify all required API keys are configured
3. **Database Issues**: Check database initialization and permissions
4. **WebSocket Connection**: Confirm WebSocket URL configuration
5. **LLM Provider Issues**: Validate API keys and model availability

### Getting Help

- **ğŸ“ GitHub Issues**: [Create an issue](https://github.com/your-repo/issues)
- **ğŸ’¬ Discussions**: [Join the discussion](https://github.com/your-repo/discussions)
- **ğŸ“– Documentation**: Check the comprehensive docs
- **ğŸ” Search**: Look through existing issues and solutions

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[LangChain Team](https://langchain.com)** - For the powerful AI framework
- **[FastAPI](https://fastapi.tiangolo.com)** - For the excellent web framework
- **[React Team](https://reactjs.org)** - For the modern frontend framework
- **[OpenAI](https://openai.com)** - For advanced language models
- **[HuggingFace](https://huggingface.co)** - For transformer models and tools
- **Open Source Community** - For the amazing ecosystem of tools

---

<div align="center">

**â­ If this project helps you, please give it a star! â­**

[ğŸ“š Documentation](./docs) â€¢ [ğŸ› Report Bug](https://github.com/your-repo/issues) â€¢ [âœ¨ Request Feature](https://github.com/your-repo/issues) â€¢ [ğŸ’¬ Discuss](https://github.com/your-repo/discussions)

Made with â¤ï¸ by the Smart AI Assistant team

</div> 